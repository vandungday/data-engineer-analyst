import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression

from salary_visualization import SalaryVisualization
from salary_prediction import SalaryPrediction

class SalaryAnalysis:
    @staticmethod
    def format_currency(amount):
        """Format s·ªë ti·ªÅn v·ªõi d·∫•u ph·∫©y v√† k√Ω hi·ªáu USD"""
        return f"${amount:,.2f}"

    @staticmethod
    def format_number(number):
        """Format s·ªë v·ªõi d·∫•u ph·∫©y"""
        return f"{number:,.2f}"

    @staticmethod
    def print_header(title, width=80):
        """In header ƒë·∫πp v·ªõi vi·ªÅn"""
        print("\n" + "=" * width)
        print(f"{title:^{width}}")
        print("=" * width)

    @staticmethod
    def print_section(title, width=50):
        """In section header"""
        print(f"\n{title}")
        print("-" * width)

    @staticmethod
    def print_metrics_table(metrics_dict, title="MODEL PERFORMANCE METRICS"):
        """In b·∫£ng metrics ƒë·∫πp"""
        print(f"\n{title}")
        print("-" * 60)
        print(f"{'Metric':<20} {'Train':<15} {'Test':<15}")
        print("-" * 60)

        for metric_name, values in metrics_dict.items():
            if isinstance(values, dict) and 'train' in values and 'test' in values:
                train_val = values['train']
                test_val = values['test']
                if 'R¬≤' in metric_name:
                    print(f"{metric_name:<20} {train_val:<15.4f} {test_val:<15.4f}")
                else:
                    print(f"{metric_name:<20} {SalaryAnalysis.format_number(train_val):<15} {SalaryAnalysis.format_number(test_val):<15}")
        print("-" * 60)
        
    def __init__(self, data_path):
        """
        Kh·ªüi t·∫°o v·ªõi ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu
        
        Args:
            data_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu
            df (DataFrame): D·ªØ li·ªáu g·ªëc
            df_processed (DataFrame): D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
            scaler (StandardScaler): ƒê·ªëi t∆∞·ª£ng chu·∫©n h√≥a d·ªØ li·ªáu
            label_encoders (dict): Dictionary ch·ª©a c√°c LabelEncoder cho c√°c c·ªôt categorical
            model (LinearRegression): ƒê·ªëi t∆∞·ª£ng m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh
            X_train, X_test, y_train, y_test (DataFrame/Series): D·ªØ li·ªáu hu·∫•n luy·ªán v√† ki·ªÉm tra
        """
        self.data_path = data_path 
        self.df = None 
        self.df_processed = None 
        self.scaler = None 
        self.label_encoders = {}
        self.model = None 
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
    def load_data(self):
        self.df = pd.read_csv(self.data_path)
        print(f"\n2. TH√îNG TIN C∆† B·∫¢N V·ªÄ D·ªÆ LI·ªÜU:")
        print("-" * 30)
        print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu: {self.df.shape}")
        print(f"S·ªë d√≤ng: {self.df.shape[0]}")
        print(f"S·ªë c·ªôt: {self.df.shape[1]}")
        
        return self.df
    
    def explore_data(self):
        """
        Kh√°m ph√° v√† t√≥m l∆∞·ª£c d·ªØ li·ªáu
        """
        print("\n3. T√ìM L∆Ø·ª¢C D·ªÆ LI·ªÜU:")
        print("-" * 30)
        
        # Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n
        print("Th√¥ng tin c√°c c·ªôt:")
        print(self.df.info())
        
        print("\nTh·ªëng k√™ m√¥ t·∫£:")
        print(self.df.describe())
        
        print("\nKi·ªÉm tra gi√° tr·ªã thi·∫øu:")
        missing_values = self.df.isnull().sum()
        print(missing_values[missing_values > 0])
        if missing_values.sum() == 0:
            print("Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu trong d·ªØ li·ªáu")
        
        # Ph√¢n t√≠ch c√°c job title ch·ª©a t·ª´ "data"
        print("\n4. PH√ÇN T√çCH JOB TITLE CH·ª®A T·ª™ 'DATA':")
        print("-" * 30)
        data_jobs = self.df[self.df['job_title'].str.contains('Data', case=False, na=False)]
        job_counts = data_jobs['job_title'].value_counts()

        print("Top 20 job title ch·ª©a 'Data':")
        print(job_counts.head(20))

        # L·ªçc c√°c job xu·∫•t hi·ªán > 30 l·∫ßn
        frequent_jobs = job_counts[job_counts > 30]
        print(f"\nC√°c job xu·∫•t hi·ªán > 30 l·∫ßn ({len(frequent_jobs)} lo·∫°i):")
        print(frequent_jobs)
        
        return data_jobs, frequent_jobs
    
    def clean_data(self):
        print("\n6. L√ÄM S·∫†CH D·ªÆ LI·ªÜU:")
        print("-" * 30)

        # T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω
        self.df_processed = self.df.copy()

        # L·ªçc c√°c job ch·ª©a t·ª´ "data" v√† xu·∫•t hi·ªán > 30 l·∫ßn
        data_jobs = self.df_processed[self.df_processed['job_title'].str.contains('Data', case=False, na=False)]
        job_counts = data_jobs['job_title'].value_counts()
        frequent_jobs = job_counts[job_counts > 30].index.tolist()

        print(f"C√°c job ch·ª©a 'Data' xu·∫•t hi·ªán > 30 l·∫ßn: {frequent_jobs}")

        # L·ªçc d·ªØ li·ªáu ch·ªâ gi·ªØ l·∫°i c√°c job frequent
        self.df_processed = self.df_processed[
            self.df_processed['job_title'].isin(frequent_jobs)
        ]

        print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi l·ªçc job titles: {self.df_processed.shape}")

        # Min, Max, Median, Std, Mean
        print(f"Min = {self.df_processed['salary_in_usd'].min():.0f}")
        print(f"Max = {self.df_processed['salary_in_usd'].max():.0f}")
        print(f"Median = {self.df_processed['salary_in_usd'].median():.0f}")
        print(f"Mean = {self.df_processed['salary_in_usd'].mean():.0f}")
        print(f"Std = {self.df_processed['salary_in_usd'].std():.0f}")

        # Lo·∫°i b·ªè c√°c outliers (s·ª≠ d·ª•ng IQR method)
        Q1 = self.df_processed['salary_in_usd'].quantile(0.25)
        Q3 = self.df_processed['salary_in_usd'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        print(f"Gi·ªõi h·∫°n outliers: [{lower_bound:.0f}, {upper_bound:.0f}]")

        # Lo·∫°i b·ªè outliers
        outliers_count = len(self.df_processed[
            (self.df_processed['salary_in_usd'] < lower_bound) |
            (self.df_processed['salary_in_usd'] > upper_bound)
        ])

        self.df_processed = self.df_processed[
            (self.df_processed['salary_in_usd'] >= lower_bound) &
            (self.df_processed['salary_in_usd'] <= upper_bound)
        ]

        print(f"ƒê√£ lo·∫°i b·ªè {outliers_count} outliers")
        print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu cu·ªëi c√πng: {self.df_processed.shape}")

        # Th√™m c·ªôt ph√¢n lo·∫°i US vs Non-US
        self.df_processed['is_us'] = (self.df_processed['company_location'] == 'US').astype(int)

        # T·∫°o c√°c c·ªôt encoded t·∫°m th·ªùi ƒë·ªÉ s·ª≠ d·ª•ng trong visualization
        categorical_columns = ['experience_level', 'employment_type', 'job_title', 'company_size']

        for col in categorical_columns:
            le = LabelEncoder()
            self.df_processed[col + '_encoded'] = le.fit_transform(self.df_processed[col])

        return self.df_processed

    def preprocess_data(self):
        """
        Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu: Encoding, Normalization, Feature Selection
        """
        print("\n7. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU:")
        print("-" * 30)

        # L√†m s·∫°ch d·ªØ li·ªáu
        self.clean_data()

        # T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω
        df_prep = self.df_processed.copy()

        # 1. Label Encoding cho c√°c bi·∫øn categorical
        categorical_columns = ['experience_level', 'employment_type', 'job_title', 'company_size']

        print("Th·ª±c hi·ªán Label Encoding cho c√°c bi·∫øn categorical:")
        for col in categorical_columns:
            le = LabelEncoder()
            df_prep[col + '_encoded'] = le.fit_transform(df_prep[col])
            self.label_encoders[col] = le
            print(f"- {col}: {len(le.classes_)} categories")

        # 2. Ch·ªçn features cho m√¥ h√¨nh
        feature_columns = [
            'work_year', 'experience_level_encoded', 'employment_type_encoded',
            'job_title_encoded', 'remote_ratio', 'company_size_encoded', 'is_us'
        ]

        X = df_prep[feature_columns]
        y = df_prep['salary_in_usd']

        print(f"\nS·ªë l∆∞·ª£ng features: {len(feature_columns)}")
        print(f"Features: {feature_columns}")

        # 3. Chia d·ªØ li·ªáu train/test
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        print(f"\nK√≠ch th∆∞·ªõc t·∫≠p train: {self.X_train.shape}")
        print(f"K√≠ch th∆∞·ªõc t·∫≠p test: {self.X_test.shape}")

        # 4. B·ªè qua chu·∫©n h√≥a ·ªü ƒë√¢y - s·∫Ω chu·∫©n h√≥a sau khi feature selection
        print("\n8. CHU·∫®N H√ìA D·ªÆ LI·ªÜU:")
        print("-" * 30)
        print("S·∫Ω th·ª±c hi·ªán chu·∫©n h√≥a sau khi feature selection ƒë·ªÉ ƒë·∫£m b·∫£o consistency")

        # 5. Feature Selection
        print("\n9. TR√çCH CH·ªåN THU·ªòC T√çNH:")
        print("-" * 30)

        # S·ª≠ d·ª•ng SelectKBest v·ªõi f_regression
        selector = SelectKBest(score_func=f_regression, k=5)  # Ch·ªçn 5 features t·ªët nh·∫•t
        X_train_selected = selector.fit_transform(self.X_train, self.y_train)
        X_test_selected = selector.transform(self.X_test)

        # L·∫•y t√™n c√°c features ƒë∆∞·ª£c ch·ªçn
        selected_features = [feature_columns[i] for i in selector.get_support(indices=True)]
        feature_scores = selector.scores_[selector.get_support()]

        print("Top 5 features ƒë∆∞·ª£c ch·ªçn:")
        for feature, score in zip(selected_features, feature_scores):
            print(f"- {feature}: {score:.2f}")

        self.selected_features = selected_features
        self.feature_selector = selector

        # L∆∞u d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c feature selection ƒë·ªÉ s·ª≠ d·ª•ng trong training
        self.X_train_selected = X_train_selected
        self.X_test_selected = X_test_selected

        return {
            'X_train_selected': self.X_train_selected,
            'X_test_selected': self.X_test_selected,
            'selected_features': self.selected_features,
            'feature_selector': self.feature_selector
        }

    def train_models(self):
        print("\n1. HU·∫§N LUY·ªÜN M√î H√åNH H·ªíI QUY TUY·∫æN T√çNH:")
        print("-" * 40)
        print("S·ª≠ d·ª•ng Z-Score Normalization v·ªõi 5 features ƒë∆∞·ª£c ch·ªçn")

        # T·∫°o scaler m·ªõi cho 5 features ƒë∆∞·ª£c ch·ªçn
        self.scaler = StandardScaler()
        X_train_selected_normalized = self.scaler.fit_transform(self.X_train_selected)
        X_test_selected_normalized = self.scaler.transform(self.X_test_selected)

        print(f"ƒê√£ chu·∫©n h√≥a {self.X_train_selected.shape[1]} features ƒë∆∞·ª£c ch·ªçn")

        # T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi 5 features ƒë∆∞·ª£c ch·ªçn
        self.model = LinearRegression()
        self.model.fit(X_train_selected_normalized, self.y_train)

        # D·ª± ƒëo√°n
        self.y_train_pred = self.model.predict(X_train_selected_normalized)
        self.y_test_pred = self.model.predict(X_test_selected_normalized)

        # L∆∞u d·ªØ li·ªáu normalized ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y
        self.X_train_selected_normalized = X_train_selected_normalized
        self.X_test_selected_normalized = X_test_selected_normalized

        # T√≠nh c√°c metrics (R¬≤, MSE, RMSE, MAE)
        self.train_r2 = r2_score(self.y_train, self.y_train_pred)
        self.test_r2 = r2_score(self.y_test, self.y_test_pred)
        self.train_mse = mean_squared_error(self.y_train, self.y_train_pred)
        self.test_mse = mean_squared_error(self.y_test, self.y_test_pred)
        self.train_mae = mean_absolute_error(self.y_train, self.y_train_pred)
        self.test_mae = mean_absolute_error(self.y_test, self.y_test_pred)
        self.train_rmse = np.sqrt(self.train_mse)
        self.test_rmse = np.sqrt(self.test_mse)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi format ƒë·∫πp
        metrics = {
            'R¬≤ Score': {'train': self.train_r2, 'test': self.test_r2},
            'MSE': {'train': self.train_mse, 'test': self.test_mse},
            'RMSE': {'train': self.train_rmse, 'test': self.test_rmse},
            'MAE': {'train': self.train_mae, 'test': self.test_mae}
        }

        self.print_metrics_table(metrics, "K·∫æT QU·∫¢ M√î H√åNH Z-SCORE NORMALIZATION")

        # L∆∞u k·∫øt qu·∫£ ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
        self.model_results = {
            'zscore': {
                'model': self.model,
                'train_r2': self.train_r2,
                'test_r2': self.test_r2,
                'train_mse': self.train_mse,
                'test_mse': self.test_mse,
                'train_rmse': self.train_rmse,
                'test_rmse': self.test_rmse,
                'train_mae': self.train_mae,
                'test_mae': self.test_mae,
                'y_train_pred': self.y_train_pred,
                'y_test_pred': self.y_test_pred
            }
        }

        self.best_method = 'zscore'
        self.best_model = self.model

        return self.model_results

    def analyze_results(self):
        """
        Ph√¢n t√≠ch chi ti·∫øt k·∫øt qu·∫£ m√¥ h√¨nh
        """
        self.print_section("2. PH√ÇN T√çCH CHI TI·∫æT M√î H√åNH", 50)

        # Hi·ªÉn th·ªã h·ªá s·ªë h·ªìi quy
        print(f"Ph∆∞∆°ng ph√°p chu·∫©n h√≥a: Z-Score Normalization")
        print(f"Intercept (Œ≤‚ÇÄ): {self.format_currency(self.model.intercept_)}")

        print(f"\n{'H·ªÜ S·ªê H·ªíI QUY (Œ≤·µ¢) - 5 FEATURES ƒê∆Ø·ª¢C CH·ªåN'}")
        print("-" * 50)
        # S·ª≠ d·ª•ng t√™n features ƒë√£ ƒë∆∞·ª£c ch·ªçn
        selected_feature_names = self.selected_features
        for i, (feature, coef) in enumerate(zip(selected_feature_names, self.model.coef_)):
            print(f"  {feature:<25}: {self.format_number(coef)}")

        # Ph√¢n t√≠ch t·∫ßm quan tr·ªçng c·ªßa features
        feature_importance = abs(self.model.coef_)
        feature_importance_normalized = feature_importance / feature_importance.sum() * 100

        print(f"\n{'T·∫¶M QUAN TR·ªåNG C·ª¶A 5 FEATURES ƒê∆Ø·ª¢C CH·ªåN'}")
        print("-" * 50)
        # S·∫Øp x·∫øp theo t·∫ßm quan tr·ªçng
        importance_data = list(zip(selected_feature_names, feature_importance_normalized))
        importance_data.sort(key=lambda x: x[1], reverse=True)

        for feature, importance in importance_data:
            bar_length = int(importance / 5)  # Scale for visualization
            bar = "‚ñà" * bar_length
            print(f"  {feature:<25}: {importance:>6.2f}% {bar}")

        # Th√™m ph·∫ßn ƒë√°nh gi√° m√¥ h√¨nh
        self.print_model_evaluation()

    def print_model_evaluation(self):
        """
        In ph·∫ßn ƒë√°nh gi√° v√† gi·∫£i th√≠ch k·∫øt qu·∫£ m√¥ h√¨nh
        """
        print(f"\n{'ƒê√ÅNH GI√Å M√î H√åNH'}")
        print("-" * 40)

        # ƒê√°nh gi√° R¬≤
        if self.test_r2 >= 0.7:
            r2_assessment = "T·ªët - M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c ph·∫ßn l·ªõn bi·∫øn thi√™n"
        elif self.test_r2 >= 0.5:
            r2_assessment = "Kh√° - M√¥ h√¨nh c√≥ kh·∫£ nƒÉng d·ª± ƒëo√°n ch·∫•p nh·∫≠n ƒë∆∞·ª£c"
        elif self.test_r2 >= 0.3:
            r2_assessment = "Trung b√¨nh - M√¥ h√¨nh c√≥ m·ªôt s·ªë kh·∫£ nƒÉng d·ª± ƒëo√°n"
        else:
            r2_assessment = "Y·∫øu - M√¥ h√¨nh c·∫ßn c·∫£i thi·ªán ƒë√°ng k·ªÉ"

        print(f"‚Ä¢ R¬≤ Score ({self.test_r2:.4f}): {r2_assessment}")
        print(f"‚Ä¢ M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c {self.test_r2*100:.1f}% bi·∫øn thi√™n c·ªßa m·ª©c l∆∞∆°ng")

        # ƒê√°nh gi√° RMSE
        avg_salary = np.mean(self.y_test)
        rmse_percentage = (self.test_rmse / avg_salary) * 100
        print(f"‚Ä¢ RMSE: {self.format_currency(self.test_rmse)} ({rmse_percentage:.1f}% c·ªßa m·ª©c l∆∞∆°ng trung b√¨nh)")

    def run_all_analysis(self):
        # B∆∞·ªõc 1: Kh√°m ph√° d·ªØ li·ªáu
        self.load_data()
        self.explore_data()

        # B∆∞·ªõc 2: L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
        self.clean_data()
        self.preprocess_data()

        # B∆∞·ªõc 3: Hu·∫•n luy·ªán v√† ph√¢n t√≠ch m√¥ h√¨nh
        self.train_models()
        self.analyze_results()

        self.print_final_dashboard()

        # B∆∞·ªõc 5: D·ª± b√°o cho c√°c k·ªãch b·∫£n
        # predictor = SalaryPrediction()

        # input_data = {
        #     'work_year': 2024,
        #     'experience_level': 'MI',
        #     'employment_type': 'FT',
        #     'job_title': 'Data Engineer',
        #     'is_usa': 1  # 1 = US, 0 = Non-US
        # }
        # predictor.predict_salary(
        #     input_data,
        #     self.model,
        #     self.scaler,
        #     self.label_encoders,
        #     self.selected_features,
        #     self.feature_selector,
        #     self.test_mae
        # )

        # predictor.visualize_predictions(
        #     self.model,
        #     self.scaler,
        #     self.label_encoders,
        #     self.selected_features,
        #     self.feature_selector,
        #     self.test_mae
        # )

        # B∆∞·ªõc 6: V·∫Ω bi·ªÉu ƒë·ªì
        # SalaryVisualization(self).draw_visualize()

        # Return k·∫øt qu·∫£ cu·ªëi c√πng
        return self

    def print_final_dashboard(self):
        """
        In dashboard t·ªïng k·∫øt cu·ªëi c√πng
        """
        self.print_header("üìä DASHBOARD T·ªîNG K·∫æT K·∫æT QU·∫¢ PH√ÇN T√çCH", 80)

        # Th√¥ng tin d·ªØ li·ªáu
        print(f"\n{'üìã TH√îNG TIN D·ªÆ LI·ªÜU'}")
        print("-" * 50)
        print(f"‚Ä¢ T·ªïng s·ªë m·∫´u ban ƒë·∫ßu: {len(self.df):,}")
        print(f"‚Ä¢ S·ªë m·∫´u sau x·ª≠ l√Ω: {len(self.df_processed):,}")
        print(f"‚Ä¢ S·ªë lo·∫°i c√¥ng vi·ªác: {len(self.df_processed['job_title'].unique())}")
        print(f"‚Ä¢ Kho·∫£ng l∆∞∆°ng: {self.format_currency(self.df_processed['salary_in_usd'].min())} - {self.format_currency(self.df_processed['salary_in_usd'].max())}")

        # Hi·ªáu su·∫•t m√¥ h√¨nh
        print(f"\n{'üéØ HI·ªÜU SU·∫§T M√î H√åNH'}")
        print("-" * 50)
        print(f"‚Ä¢ ƒê·ªô ch√≠nh x√°c (R¬≤): {self.test_r2:.1%}")
        print(f"‚Ä¢ Sai s·ªë trung b√¨nh (MAE): {self.format_currency(self.test_mae)}")
        print(f"‚Ä¢ Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh (MSE): {self.format_currency(self.test_mse)}")
        print(f"‚Ä¢ Sai s·ªë b√¨nh ph∆∞∆°ng g·ªëc (RMSE): {self.format_currency(self.test_rmse)}")

        # Top features quan tr·ªçng (t·ª´ 5 features ƒë√£ ƒë∆∞·ª£c ch·ªçn)
        selected_feature_names = self.selected_features
        feature_importance = abs(self.model.coef_)
        feature_importance_normalized = feature_importance / feature_importance.sum() * 100
        importance_data = list(zip(selected_feature_names, feature_importance_normalized))
        importance_data.sort(key=lambda x: x[1], reverse=True)

        print(f"\n{'üîç TOP 5 Y·∫æU T·ªê QUAN TR·ªåNG NH·∫§T (ƒê√É ƒê∆Ø·ª¢C CH·ªåN)'}")
        print("-" * 50)
        for i, (feature, importance) in enumerate(importance_data, 1):
            print(f"{i}. {feature}: {importance:.1f}%")

        self.print_header("üéâ HO√ÄN TH√ÄNH PH√ÇN T√çCH!", 80)

def main():
    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file d·ªØ li·ªáu
    data_path = "data-salary.csv"

    try:
        analyzer = SalaryAnalysis(data_path)
        analyzer.run_all_analysis()

    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file {data_path}")
        print("Vui l√≤ng ki·ªÉm tra ƒë∆∞·ªùng d·∫´n file d·ªØ li·ªáu")
    except Exception as e:
        print(f"L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {str(e)}")


if __name__ == "__main__":
    main()
